---
title: "MovieLens Capstone Project"
author: "Astrid Melhado Dyer"
date: "1/2/2020"
output: 
  pdf_document:
    toc: true
    toc_depth: 4
    number_sections: true
---

The purpose of the present assignment is to create a recommendation system using a reduced version of the MovieLens data set, and if possible to attain a RMSE <= 0.8649. The approach taken is rather opposite than the one used in the submission of the second capstone project of the course: Choose Your Own!  Instead of conducting a spot-check algorithm or ensembling predictions, I rely heavily on the **recommenderlab** package to design a Collaborative Filtering Recommender Model.

Due to the pedagogic nature of the report, R codes are enclosed except when they are deemed repetitive, or do not contribute to show R proficiency. However RMD and R file maybe consulted to evaluate the entire code.

## Training and Validation Set

Data Partition has been already taken care by the assignment prompt, and since this is a curated data set, there is little left to do regarding data cleaning.  Thus, the real challenge of the assignment is how to deal with a large data set in a computer wise efficient manner.


```{r eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, comment=NA}
# code given by prompt
# Note: this process could take a couple of minutes

library(tidyverse)
library(caret)
library(data.table) 
library(recommenderlab)
library(Matrix) 
library(kableExtra)
library(ggplot2)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip


dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))



movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

The dimensions of the **edx** data set evidence the size of the call:

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
dim(edx)
```


Dimensions of the validation set as follows:

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
dim(validation)
```

## Data Preparation

In order to utilize the functions provided by the recommenderlab package, it is necessary to convert the data in a **realRatingMatrix”**. An S4 object that contains sparse matrices; hence, the first step is to transform the **edx** data set in a sparse matrix, and thereafter coerce them into a realRatingMatrix.


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
ratings_m <- sparseMatrix(i = as.integer(as.factor(edx$userId)),
                          j = as.integer(as.factor(edx$movieId)),
                          x = edx$rating)

dimnames(ratings_m) <- list(
  user=paste('u', 1:length(unique(edx$userId)), sep=''),
  item=paste('m', 1:length(unique(edx$movieId)), sep=''))

class(ratings_m)
rRM <- as(ratings_m, "realRatingMatrix")
class(rRM)
```

### Data Exploration

A glimpse of the matrix structure:

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
slotNames(rRM)
```

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
dim(rRM@data)
```

### Selecting Data that Matter

As it will be shown later, there is a large variation across users. Measures must be taken to avoid the potential bias caused by the lack of data generated by movies viewed only a couple of times. Likewise, distortion could come from users who have rated a few movies only. It is wise to avoid both cases by appointing a threshold. In early stages, the following rule of thumb might be used and polished thereupon: **select movies that have been watched at least 100 times and users who have given their rating to at least 50 movies.**


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
rRM_mod <- rRM[rowCounts(rRM) > 50, colCounts(rRM) > 100]
rRM_mod
```

Or we directly proceed to employ the quantile function. 


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
min_movies1 <- quantile(rowCounts(rRM), 0.90) 
print(min_movies1)
```


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
min_users1 <- quantile(colCounts(rRM), 0.90)
print(min_users1)
```

Either way, the idea is to keep only the relevant data.

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
rRM_mod2 <- rRM[rowCounts(rRM) > min_movies1,
                colCounts(rRM) > min_users1]
rRM_mod2

```

### Normalization 

What about if there are users that just give high or low ratings to all their movies? This source of bias could be easily removed by normalizing the data aka making the average rating for each user 0 by means of the built-in normalize function.

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
#Normalizing the data
rRM_mod2_norm <- normalize(rRM_mod2)
sum(rowMeans(rRM_mod2_norm) > 0.00001) #Testing for Normality
```

Originally, the rating was an integer 1 to 5. After the normalization, we are dealing with continuous data, and the rating could be any number between -5 and 5.

**Normalize**  is a default parameter within the functions to come; thus, there is no need to run the process except for illustration's puposes as we have done here.

## Understanding Ratings

By momentarily, converting the matrix into a vector, we are able to explore the ratings. 

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
v_ratings <- as.vector(rRM_mod2@data) 
unique(v_ratings) 
#count the occurrences 
t_ratings <- table(v_ratings) 
kable(t_ratings)

```

Let us remove the ratings equal to 0 since they represent missing values according to the documentation, and convert the remaining ones into factors in order to build a frequency plot via the ggplot2 package.

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
#0 Means no rating, turn into factors to plot
v_ratings <- v_ratings[v_ratings != 0]
v_ratings <- factor(v_ratings)
head(v_ratings)
str(v_ratings)
#plot
pl_v_ratings <- ggplot(as_tibble(v_ratings), aes(x=value, fill = ..x..))+
  geom_histogram(stat = "count")+
  ggtitle("Ratings Distribution")+
  theme(plot.title = element_text(hjust = 0.5))+
  scale_fill_gradient2(low = "green", high = "blue")+
  guides(fill=FALSE)
pl_v_ratings


```

Most common rating is 4, and most of them occur above 2.

### Ratings Distribution

A plot of the average rating distribution might provide an overall view that enables the decision making process. The highest bar gravitates around 3.5, and there are few ratings that are either 1 or 5. So, removing them as we did in the section **selecting data that matter**,  it is the way to go.

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
avg_ratings_per_user <- as_tibble(rowMeans(rRM_mod2))
#head(avg_ratings_per_user)

pl_avg_ratings_per_user <- ggplot(avg_ratings_per_user, aes(x=value, fill= ..x..))+ 
  geom_histogram(binwidth = 0.10)+
  ggtitle("Distribution of the Average Rating per User") + 
  theme(plot.title = element_text(hjust = 0.5))+
  xlab("Average Rating per Users")+
  scale_fill_gradient("Average Ratings", low = "green", high = "blue")+
  geom_vline(xintercept=mean(avg_ratings_per_user$value), color="black", size=2)

pl_avg_ratings_per_user

```

```{r eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, comment=NA}
kable(summary(avg_ratings_per_user))
```

## The Notion of  Distance

In plain words, the basic notion that lays at the foundations of **Collaborative Filtering Recommender Systems” is that if two users share the same interest on a given item in the past, they will, most likely, have similar taste in the future, Hence, the key question is how to calculate the similarity between users and items in order to generate predictions for new users and new items whose data is unknown. 

Similarity measures like Euclidian distance, Cosine and Pearson correlation become a capital notion for this type of model.

See the following two matrices depicting distance between users.

### Cosine similarity

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
similarity_cosine <- similarity(rRM_mod2[1:5, ], method = "cosine", which = "users")
#class(similarity_cosine)
kable(as.matrix(similarity_cosine))
```

### Pearson Similiraty

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
similarity_pearson <- similarity(rRM_mod2_norm[1:5, ], method = "pearson", which = "users")
#class(similarity_pearson)
kable(as.matrix(similarity_pearson))

```


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
par(mfrow = c(1, 2))
im_cosine <- image(as.matrix(similarity_cosine), main = "User Similarity - method = Cosine")
im_pearson <- image(as.matrix(similarity_pearson), main = "User Similarity - method = Pearson")

```

The deeper the color, the farther the distance. Of course, the light diagonal signals 0 distance between same user. 

So far, the Pearson method seems more accurate in indicating similarities between users. Let us see, if that hint is being confirmed by further testing.


## Constructing the Model

To tackle the assignment, I will start by building a simple individual model to grasp understanding, and depart from there. I have selected an **IBCF** model or Item-Based Collaborative Filtering meaning: given two items, an index is calculated by dividing the number of users purchasing both items by the number of users purchasing at least one of them.

### Defining Parameters

Although training and data set are given by the assignment prompt, each of them, when required, is partitioned according the guidelines of the **recommenderlab** package by means of the **evaluationScheme()** function to stay true to the selected package. In this first attempt, the split method will be used.

Furthermore, for each user in the test set, it is necessary to indicate how many items to use in otder to generate recommendations. The remaining ones will be employed to test model accuracy. This parameter should be lower than the minimum number purchased by any user; otherwise we might end up with users without items to test the model. That is the function of the piece of code: **min(rowCounts())**

Also, we need to define what constitutes good and bad items. This brain-teaser is easily solved by setting a threshold by selecting the rating’s media. 

Finally, I set to 1 the numbers of times to run the evaluation.



```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
percentage_training <- 0.8
min(rowCounts(rRM_mod2))#37
items_to_keep <- 25
rating_threshold <- 3
n_eval <- 1
```

### Splitting the Data

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
# split
eval_sets_split <- evaluationScheme(data = rRM_mod2, method = "split", 
                              train = percentage_training, given = items_to_keep, 
                              goodRating = rating_threshold, k = n_eval)

```

For good order's sake, we check the different outcomes.


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
##Checking eval_sets
getData(eval_sets_split, "train")
```

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
#same number of users
getData(eval_sets_split, "known")#1396 x 1068 rating matrix of class ‘realRatingMatrix’ with 34900 ratings.
```

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
getData(eval_sets_split, "unknown")#1396 x 1068 rating matrix of class ‘realRatingMatrix’ with 427043 ratings.
```

We make sure that around 20% of the total data is in the test sets

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
#There should be about 20 percent of data in the test set:
nrow(getData(eval_sets_split, "known")) / nrow(rRM_mod2)#0.2000573
```

We check that items to keeps is equal amount of items for each user.

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
#Let's see how many items we have for each user in the known set. It should be equal to items_to_keep
unique(rowCounts(getData(eval_sets_split, "known")))#25
```

As expected, the number of items by users varies a lot. The distribution is right skewed. The black line indicates the mean. Normalization is a must.

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
dat_unknown <- as.tibble(rowCounts(getData(eval_sets_split, "unknown"))) 
head(dat_unknown)
pl_unknown <- ggplot(dat_unknown, aes(x=value, fill=..x..))+
  geom_histogram(binwidth = 10) +
  theme(plot.title = element_text(hjust = 0.5))+
  ggtitle("unknown items by the users")+
  xlab("Unknown Items")+
  scale_fill_gradient("Items", low = "green", high = "blue")+
  geom_vline(xintercept=mean(dat_unknown$value), color="black", size=2)
pl_unknown

```



As an alternative to the **split** method, we use **k-folds** which seems to perform more accurately; although, it takes more computer time.

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
n_fold <- 4 
eval_sets_kfold <- evaluationScheme(data = rRM_mod2, method = "cross-validation", 
                                    k = n_fold, given = items_to_keep, goodRating = rating_threshold)
```

### Setting the Model

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
##single model
model_parameter <- NULL
```

Please note that when parameter are set to null,default parameters of the function will be applied.

### Creating the Recommender

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}

eval_recommender <- Recommender(data = getData(eval_sets_kfold, "train"), 
                                method = "IBCF", parameter = model_parameter)
items_to_recommend <- 10
```

### Predicting 

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
eval_prediction <- predict(object = eval_recommender, newdata = getData(eval_sets_kfold, "known"),
                           n = items_to_recommend, type = "ratings") 
class(eval_prediction)
```

### Predictions Distribution

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
d_prediction <- as.tibble(rowCounts(eval_prediction))
kable(head(d_prediction))
```
So far, so good, our model predicts 4 as the most common rating which is consistent with our findings so far. 

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
pl_d_prediction <- ggplot(d_prediction, aes(x =value, fill = ..x..))+
  geom_histogram(binwidth = 10) + 
  ggtitle("Distribution of Predicted Ratings per User")+
  theme(plot.title = element_text(hjust = 0.5))+
    scale_fill_gradient("Items", low = "green", high = "blue")+
  geom_vline(xintercept=mean(d_prediction$value), color="black", size=2)
pl_d_prediction
```

## Accuracy

First, we evaluate the accuracy per user before evaluating the model accuracy via the RMSE as requested by the assignment prompt. To this end is necessary to have in mind the concepts summarized hereunder. Also, do note that in this context **items** equate to **movies**:

+ **(TP) True Positives**: recommended items that have been purchased.
+ **(FP) False Positives**: recommended items that have not been purchased.
+ **(FN) False Negatives**: not recommended items that have been purchased.
+ **(TN) True Negatives**: not recommended items that have not been purchased.
+ **(TPR) True Positive Rate**: percentage of purchase items that have been recommended =TP/(TP+FN)
+ **(FPR) False Positive Rate**: percentage of non-purchase items that have been recommended = FP/(FP+TN)


### Accuracy per User

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
eval_accuracy <- calcPredictionAccuracy( 
  x = eval_prediction, data = getData(eval_sets_kfold, "unknown"), byUser = 
    TRUE) 
#head(eval_accuracy)
tail(eval_accuracy)
```

Out of the 6 displayed observations, just 2 approximates to the RMSE target which implies testing alternative models and optimizing numeric parameters.


### Distribution of RMSE

A well-depicted graph is always helpful for getting the bigger picture. The mean gravitates between 1.30 and 1.40 RMSE, far from desired value.


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
d_accuracy <- as.tibble(eval_accuracy[, "RMSE"])
kable(head(d_accuracy, 10))

pl_d_accuracy <- ggplot(d_accuracy, aes(x=value, fill = ..x..))+ 
  geom_histogram(binwidth = 0.1)+
  ggtitle("Distribution of the RMSE by user")+
  theme(plot.title = element_text(hjust = 0.5))+
  scale_fill_gradient("RMSE", low = "green", high = "blue")+
  geom_vline(xintercept=mean(d_accuracy$value), color="black", size=2)+
  scale_x_continuous(breaks = seq(0, 4, 0.25))

pl_d_accuracy

```

### Accuracy per Model

Already warned by the previous model, it does not come as a surprise the RSME when evaluating the model. Sufficed to set the parameter **byuser** to false.

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
#Accuracy per model

eval_accuracy_model <- calcPredictionAccuracy(x = eval_prediction, data = getData(eval_sets_kfold, "unknown"),
                                              byUser = FALSE) 
eval_accuracy_model
```

### Evaluate the Recommendations

Our model performs well when dealing with true negatives, but behaves poorly when it comes to true positives.

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
#Evaluate the recommendations
results <- evaluate(x = eval_sets_kfold, method = "IBCF", n = seq(10, 100, 10))
#class(results)
kable(head(getConfusionMatrix(results)[[1]]))
```

### Aggregated Values

By summarizing the values, we get the full picture.

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
col_to_sum <- c("TP", "FP", "FN", "TN") 
aggregated_values <- Reduce("+", getConfusionMatrix(results))[, col_to_sum] 
kable(head(aggregated_values))
```

## Recommender Package

A quick view of some of the bounties of the *recommenderlab* package. Note that the package allow us to choose from several models. 

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
Available_Models <- recommenderRegistry$get_entries(dataType = "realRatingMatrix")
kable(names(Available_Models))
```

## Selecting the Most Suitable Model

Due to the nature of the data we narrow the scope to User and Item Based Collaborative filtering recommender models.

```{r eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, comment=NA}
m_list <- list( 
  IBCF_cos = list(name = "IBCF", param = list(method = "cosine")),
  IBCF_cor = list(name = "IBCF", param = list(method = "pearson")),
  UBCF_cos = list(name = "UBCF", param = list(method = "cosine")),
  UBCF_cor = list(name = "UBCF", param = list(method = "pearson")),
  random = list(name = "RANDOM", param=NULL))
n_recommendations <- c(1, 5, seq(10, 100, 10))
list_results <- evaluate(x = eval_sets_kfold, method = m_list, n = n_recommendations) 
#class(list_results)
avg_matrices <- lapply(list_results, avg)
head(avg_matrices$UBCF_cos[, 5:8])
```


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
plot(list_results, annotate = 1, legend = "topleft") 
title("ROC curve")
```


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
plot(list_results, "prec/rec", annotate = c(1,2), legend = "bottomright")
title("Precision-Recall")
```

Based on above plots, our first contestant is UBCF method = Pearson. 

## Optimizing Numeric Parameters

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
vector_k <- c(5, 10, 20, 30, 40)
models_to_evaluate_1 <- lapply(vector_k, function(k){ 
  list(name = "UBCF", param = list(method = "pearson", k = k))})
names(models_to_evaluate_1) <- paste0("UBCF_k_", vector_k)
n_recommendations <- 20
list_results_k <- evaluate(x = eval_sets_kfold, method = models_to_evaluate_1, 
                           n = n_recommendations)
plot(list_results, annotate = 1,legend = "topleft") 
title("ROC curve")
```


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
plot(list_results, "prec/rec", annotate =1, legend = "bottomright") 
title("Precision-recall")
```

It seems tha setting n_fold to 40 is where the trade-off between precision and recall are maximized.

## Testing on the Validation Set

Without further ado, we proceed to run the model on the validation set.

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
ratings_m_val <- sparseMatrix(i = as.integer(as.factor(validation$userId)),
                          j = as.integer(as.factor(validation$movieId)),
                          x = validation$rating)

dimnames(ratings_m_val) <- list(
  user=paste('u', 1:length(unique(validation$userId)), sep=''),
  item=paste('m', 1:length(unique(validation$movieId)), sep=''))

class(ratings_m_val)
rRM_val <- as(ratings_m_val, "realRatingMatrix")
class(rRM_val)
```


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
dim(rRM_val)
```

``````{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
rRM_mod2_val <- rRM[rowCounts(rRM_val) > 50, colCounts(rRM) > 100]
rRM_mod2
```


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
min(rowCounts(rRM_mod2_val))
```


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
n_fold <- 40
items_to_keep <- 10
rating_threshold <- 3
n_eval <- 1

set.seed(13)
eval_sets_kfold_val <- evaluationScheme(data = rRM_mod2_val, method = "cross-validation", 
                                    k = n_fold, given = items_to_keep, goodRating = rating_threshold)
```


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
eval_recommender_val <- Recommender(data = getData(eval_sets_kfold_val, "train"), 
                                method = "UBCF", parameter = "pearson")
```

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
model_detail <- getModel(eval_recommender_val) 
model_detail$description
```



```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
set.seed(13)
eval_prediction_val <- predict(object = eval_recommender_val, newdata = getData(eval_sets_kfold_val, "known"),
                           n = items_to_recommend, type = "ratings") 
class(eval_prediction_val)
```


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
set.seed(13)
eval_accuracy_val <- calcPredictionAccuracy( 
  x = eval_prediction_val, data = getData(eval_sets_kfold_val, "unknown"))
#head(eval_accuracy)
eval_accuracy_val
```

Although maximizing the numerical parameters mainly k-fold, and switching from item to user based collaborative filtering model have greatly improve our RSME from 1.46 to 0.96, it does not yet hit the mark.

## Why do I not achieve a more desirable RMSE?

Is it the model or the  data reduction techniques employed where I have gone amiss? 

In order to ascertain the cause of my demise, I run the model on the *MovieLense* data set encountered in the *recommenderlab* package. The data is already a realRating Matrix which implies that dimensionality reduction techniques have already wisely applied to the original data set.


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
data("MovieLense")
dim(MovieLense)
```


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
class(MovieLense)
```
```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
slotNames(MovieLense)
```


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
min_movies_ML <- quantile(rowCounts(MovieLense), 0.90) 
print(min_movies_ML)
```

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
min_users_ML <- quantile(colCounts(MovieLense), 0.90)
print(min_users_ML)
```

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
RM <- MovieLense[rowCounts(MovieLense) > min_movies_ML,
                colCounts(MovieLense) > min_users_ML]
RM
```

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
min(rowCounts(RM))
```


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
n_fold <- 40
items_to_keep <- 50
rating_threshold <- 3
n_eval <- 1

set.seed(13)
eval_sets_RM <- evaluationScheme(data = RM, method = "cross-validation", 
                                 k = n_fold, given = items_to_keep, goodRating = rating_threshold)
```


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
set.seed(13)
eval_recommender_RM <- Recommender(data = getData(eval_sets_RM, "train"), 
                                   method = "UBCF", parameter = "pearson")
```


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
set.seed(13)
eval_prediction_RM <- predict(object = eval_recommender_RM, newdata = getData(eval_sets_RM, "known"),
                              n = items_to_recommend, type = "ratings") 
class(eval_prediction_RM)
```


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
set.seed(13)
eval_accuracy_RM <- calcPredictionAccuracy(x = eval_prediction_RM,
                                           data = getData(eval_sets_RM, "unknown"),
                                           byUser = FALSE) 
eval_accuracy_RM
```

## Conclusions

Attained RSME of 0.8647487 is within the indicated target, but the results were not obtained over the validation set as dimensionality reduction techniques were, most likely, not properly applied.  Designed model proved to come across, but failure to effectively reduce the data was costly.  Although, I consider that I successfully created a recommender, the picture is far from complete without mastering tools as **PCA**, **SVD**, and others. Learning such techniques is something to look forward in the nearby future.



## Works Cited

Douglas Bates and Martin Maechler (2019). Matrix: Sparse and Dense Matrix Classes and Methods. R
  package version 1.2-18. https://CRAN.R-project.org/package=Matrix

Michael Hahsler (2019). recommenderlab: Lab for Developing and Testing Recommender Algorithms. R
  package version 0.2-5. https://CRAN.R-project.org/package=recommenderlab
  
Matt Dowle and Arun Srinivasan (2019). data.table: Extension of `data.frame`. R package version
 1.12.2. https://CRAN.R-project.org/package=data.table

Max Kuhn. Contributions from Jed Wing, Steve Weston, Andre Williams, Chris Keefer, Allan Engelhardt,
  Tony Cooper, Zachary Mayer, Brenton Kenkel, the R Core Team, Michael Benesty, Reynald Lescarbeau,
  Andrew Ziem, Luca Scrucca, Yuan Tang, Can Candan and Tyler Hunt. (2019). caret: Classification and
  Regression Training. R package version 6.0-84. https://CRAN.R-project.org/package=caret

H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.

Hao Zhu (2019). kableExtra: Construct Complex Table with 'kable' and Pipe Syntax. R package version
  1.1.0. https://CRAN.R-project.org/package=kableExtra
  
Gorakala Suresh K, Michelle Usuelli.Building a Recommendation System with R.Packt Publishing 
Birmingham,h 2015. 

